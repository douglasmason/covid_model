<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_8eiqs74scqtf-8{list-style-type:none}.lst-kix_yg2qa2ri8xlm-8>li:before{content:"\0025a0  "}ul.lst-kix_8eiqs74scqtf-7{list-style-type:none}ul.lst-kix_8eiqs74scqtf-6{list-style-type:none}ul.lst-kix_8eiqs74scqtf-5{list-style-type:none}ul.lst-kix_8eiqs74scqtf-4{list-style-type:none}.lst-kix_yg2qa2ri8xlm-6>li:before{content:"\0025cf  "}ul.lst-kix_8eiqs74scqtf-3{list-style-type:none}ul.lst-kix_8eiqs74scqtf-2{list-style-type:none}.lst-kix_yg2qa2ri8xlm-7>li:before{content:"\0025cb  "}ul.lst-kix_8eiqs74scqtf-1{list-style-type:none}ul.lst-kix_8eiqs74scqtf-0{list-style-type:none}.lst-kix_yg2qa2ri8xlm-4>li:before{content:"\0025cb  "}ul.lst-kix_is3aj51kayou-8{list-style-type:none}ul.lst-kix_is3aj51kayou-7{list-style-type:none}ul.lst-kix_is3aj51kayou-6{list-style-type:none}.lst-kix_yg2qa2ri8xlm-5>li:before{content:"\0025a0  "}.lst-kix_is3aj51kayou-4>li:before{content:"\0025cb  "}.lst-kix_is3aj51kayou-6>li:before{content:"\0025cf  "}.lst-kix_is3aj51kayou-3>li:before{content:"\0025cf  "}.lst-kix_is3aj51kayou-7>li:before{content:"\0025cb  "}.lst-kix_is3aj51kayou-0>li:before{content:"  "}.lst-kix_is3aj51kayou-2>li:before{content:"\0025a0  "}.lst-kix_is3aj51kayou-8>li:before{content:"\0025a0  "}.lst-kix_is3aj51kayou-1>li:before{content:"\0025cb  "}ul.lst-kix_is3aj51kayou-1{list-style-type:none}.lst-kix_yg2qa2ri8xlm-0>li:before{content:"  "}ul.lst-kix_is3aj51kayou-0{list-style-type:none}ul.lst-kix_is3aj51kayou-5{list-style-type:none}.lst-kix_yg2qa2ri8xlm-2>li:before{content:"\0025a0  "}ul.lst-kix_is3aj51kayou-4{list-style-type:none}ul.lst-kix_is3aj51kayou-3{list-style-type:none}.lst-kix_yg2qa2ri8xlm-3>li:before{content:"\0025cf  "}ul.lst-kix_is3aj51kayou-2{list-style-type:none}.lst-kix_yg2qa2ri8xlm-1>li:before{content:"\0025cb  "}.lst-kix_is3aj51kayou-5>li:before{content:"\0025a0  "}.lst-kix_zi1iwgjdlg90-4>li:before{content:"\0025cb  "}.lst-kix_zi1iwgjdlg90-5>li:before{content:"\0025a0  "}.lst-kix_zi1iwgjdlg90-2>li:before{content:"\0025a0  "}.lst-kix_zi1iwgjdlg90-3>li:before{content:"\0025cf  "}.lst-kix_zi1iwgjdlg90-6>li:before{content:"\0025cf  "}.lst-kix_zi1iwgjdlg90-7>li:before{content:"\0025cb  "}.lst-kix_zi1iwgjdlg90-0>li:before{content:"  "}.lst-kix_zi1iwgjdlg90-1>li:before{content:"\0025cb  "}ul.lst-kix_zi1iwgjdlg90-5{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-4{list-style-type:none}.lst-kix_8eiqs74scqtf-3>li:before{content:"\0025cf  "}.lst-kix_8eiqs74scqtf-4>li:before{content:"\0025cb  "}ul.lst-kix_yg2qa2ri8xlm-0{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-3{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-1{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-2{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-2{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-1{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-0{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-7{list-style-type:none}.lst-kix_8eiqs74scqtf-0>li:before{content:"  "}.lst-kix_8eiqs74scqtf-8>li:before{content:"\0025a0  "}ul.lst-kix_yg2qa2ri8xlm-8{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-3{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-8{list-style-type:none}.lst-kix_8eiqs74scqtf-1>li:before{content:"\0025cb  "}.lst-kix_8eiqs74scqtf-2>li:before{content:"\0025a0  "}ul.lst-kix_yg2qa2ri8xlm-4{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-7{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-5{list-style-type:none}ul.lst-kix_zi1iwgjdlg90-6{list-style-type:none}ul.lst-kix_yg2qa2ri8xlm-6{list-style-type:none}.lst-kix_zi1iwgjdlg90-8>li:before{content:"\0025a0  "}.lst-kix_8eiqs74scqtf-7>li:before{content:"\0025cb  "}.lst-kix_8eiqs74scqtf-5>li:before{content:"\0025a0  "}.lst-kix_8eiqs74scqtf-6>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c9{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:156pt;border-top-color:#000000;border-bottom-style:solid}.c6{background-color:#ffffff;color:#222222;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c17{color:#333333;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c18{color:#222222;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c15{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c21{border-spacing:0;border-collapse:collapse;margin-right:auto}.c14{padding-top:-6pt;padding-bottom:0pt;line-height:1.0;text-align:right}.c19{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c10{orphans:2;widows:2;height:11pt}.c20{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{font-size:10pt;font-style:italic;color:#222222}.c16{background-color:#ffffff;color:#222222}.c23{width:33%;height:1px}.c12{orphans:2;widows:2}.c22{color:inherit;text-decoration:inherit}.c3{font-weight:700}.c5{height:0pt}.c8{font-size:10pt}.c24{color:#222222}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c20"><h2 class="c12 c15" id="h.sa994e6imlkn"><span class="c13">Methods for Computing Uncertainty in COVID-19 Models</span></h2><p class="c11"><span>Douglas Mason</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></p><p class="c11"><span class="c2">June 25, 2020</span></p><p class="c1"><span class="c2"></span></p><a id="t.99a374f96ca6369f826efe17261dd2a94d923986"></a><a id="t.0"></a><table class="c21"><tbody><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Approach</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Pros</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Cons</span></p></td></tr><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span>Curvature at MLE</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Standard, well-known practice from classical statistics; cheap calculations</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Assumptions may be invalid for certain types of models; depends on obtaining the true MLE which isn&rsquo;t guaranteed</span></p></td></tr><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span>Drop-out Trick</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Well-known practice for obtaining uncertainty estimates from neural nets</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Only applies to neural nets and uncertainties in predictions (not parameters); still outstanding discussion of validity in certain applications</span></p></td></tr><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span>Bootstrapping</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Stable, easy to implement</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Inaccurate for estimating uncertainties in parameters that are defined by few data points; requires hand-holding regarding initial parameter guesses</span></p></td></tr><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span>Variational Bayes</span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Cheap calculations, many state-of-the-art techniques use it; robust support in various software packages</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Only obtains lower estimates of uncertainties; certain approximations (mean-field) ignore interactions among parameters; requires modern software packages</span></p></td></tr><tr class="c5"><td class="c9" colspan="1" rowspan="1"><p class="c0"><span>Direct Sampling (including &nbsp;rejection sampling and Markov-Chain Monte-Carlo variants</span><sup><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c2">)</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">The gold standard in uncertainty measurements of the complex likelihood or posterior functions</span></p></td><td class="c9" colspan="1" rowspan="1"><p class="c0"><span class="c2">Weak guarantees; MCMC is only accurate when you&rsquo;ve reached the equilibrium state, which is difficult to determine; very resource intensive</span></p></td></tr></tbody></table><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c3">Table 1:</span><span class="c2">&nbsp;Uncertainty measurement methods in increasing order of resources required to compute them</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c2">We provide a short review of common methods for estimating uncertainty in either the parameters (for interpretable models) or the predictions (either interpretable or non-interpretable) of a given model. Each approach has its strengths and weaknesses, as outlined in Table 1. Since any individual estimate of the uncertainty may be flawed, we propose to apply as many of these techniques as resources allow so we can determine, by means of agreement and knowledge of the known strengths and weaknesses, which groups of estimates are most reliable. In general, the farther we move from simple, linear-regression-like models, the more disagreement that is found in these methods.</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c2">The primary challenge in computing the uncertainties in model parameters (which are then used to compute the uncertainties in model predictions, except for the drop-out trick), is that the likelihood function which describes the fit of the observed data to a given point in parameter space is enormously complex, with valleys and peaks in many dimensions. When we describe the uncertainty of any given parameter, we must integrate the function over the remaining parameters, and when describing the uncertainty of any given prediction, we must integrate over all parameters. This intractable calculation of eminent importance has given rise to a huge field of approximation methods that are actively being researched, although some of the most practical solutions date back to the birth of scientific computing in the 1950s. </span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span>The classic approach to computing uncertainties in statistical models is to first compute the Maximum Likelihood Estimator (MLE), that is, the point in parameter space that maximizes the likelihood of the data given the model using standard optimization methods. Then, the </span><span class="c3">curvature of the likelihood function</span><span class="c2">&nbsp;is computed using numerical approximations to the second-order derivatives at the MLE (there are several sub-variants to these approximations, including computing the second-order derivatives directly, i.e., Hessian methods, or using a Taylor expansion of the Hessian matrix, i.e., Jacobian methods). Under general assumptions, the curvature gives us an approximation to the uncertainties of the model, and this approximation is exact under a standard linear regression. These methods are the heart of non-linear regression packages in R, MATLAB, Mathematica, and Python (Scipy).</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span>For models based on multi-layer perceptrons, or neural networks, that can employ drop-out regularization (the randomized dropping of inputs to perceptrons within the network), the &ldquo;</span><span class="c3">drop-out trick</span><span class="c2">&rdquo; allows us to compute uncertainties in our predictions, but not of the model parameters. This approach amounts to performing repeated inferences with a different randomization of where drop-out occurs, and is sometimes referred to as the MC (Monte-Carlo) drop-out trick. This is a relatively novel approach, and there is still outstanding discussion on its validity in various contexts, but it has enjoyed overall healthy support from the ML community.</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c3">Bootstrapping</span><span class="c2">&nbsp;is a common and extremely robust solution which requires re-sampling the training data with replacement, training a new model on each re-sample, and using the results as an approximation to the uncertainty. Bootstrapping is exact in the limit of infinite data, and therefore poorly predicts parameter uncertainties that are supported by only a few data points, and for sufficiently complex models, it can be non-trivial to determine which of the parameters are most at risk. Since bootstrapping requires calculating the MLE for each training data re-sample, it requires intermediate resources to calculate, and is sensitive to the starting guess.</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c3">Variational Bayesian Inference</span><span class="c2">&nbsp;(VBI) has enjoyed substantial attention from the ML community in the past decade, and is featured in prominent state-of-the-art commercial applications (Pyro, an implementation in Python, was recently acquired by Uber to facilitate the development of self-driving cars). It is also the basis of Dynamic Causal Models (DCM) which are discussed elsewhere in this document. At its heart, VBI approximates the likelihood or posterior functions as a multivariate Gaussian, and stronger assumptions (e.g., mean-field approximations) approximate it with only a diagonal covariance matrix, i.e., we assume there are no correlations between the parameters themselves. These strong assumptions mean that VBI only provides a lower bound of these uncertainties. Due to strong development in recent years, these methods are increasingly robust and cheap to compute, but they are also strongly approximate, so we use them with caution.</span></p><p class="c1"><span class="c2"></span></p><p class="c11"><span class="c3">Direct sampling</span><span>&nbsp;of the likelihood and posterior functions</span><span>, either through rejection sampling (sampling with a guessed distribution and rejecting based on the returned result) or Markov-Chain Monte Carlo (MCMC, along with its multitude of variations, including NUTS</span><sup><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span>, ensemble MC</span><sup><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><span>, Firefly</span><sup><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c2">, and the classic Metropolis-Hastings and Gibbs sampling algorithms), is considered the gold standard in computing model uncertainties, but these methods are also by far the most resource intensive. Moreover, it is challenging to determine when these methods have reached the equilibrium state where they are valid. Such methods are considered a last resort, but can provide valuable information to contextualize the other approaches. For example, it is common in an MCMC algorithm to explore regions that MLE optimizers may miss and to discover better MLEs, which can then be fed back into the curvature approximation.</span></p><p class="c1"><span class="c2"></span></p><p class="c1"><span class="c2"></span></p><hr class="c23"><div><p class="c0 c12"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c8">&nbsp;Koyote Science LLC, San Francisco, CA </span><span class="c8 c19"><a class="c22" href="https://www.google.com/url?q=http://www.koyotescience.com&amp;sa=D&amp;ust=1593140161860000">http://www.koyotescience.com</a></span></p><p class="c0 c10"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c8">&nbsp;</span><span class="c8 c17">J. J. More, &ldquo;The Levenberg-Marquardt Algorithm: Implementation and Theory,&rdquo; Numerical Analysis, ed. G. A. Watson, Lecture Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.<br></span></p><p class="c0 c12"><span class="c8 c24">Thacker, William Carlisle. &quot;The role of the Hessian matrix in fitting models to measurements.&quot; </span><span class="c7">Journal of Geophysical Research: Oceans</span><span class="c8 c18">&nbsp;94.C5 (1989): 6177-6196.</span></p><p class="c10 c14"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c8">&nbsp;</span><span class="c16 c8">Gal, Y. and Ghahramani, Z., 2016, June. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In </span><span class="c7">In</span><span class="c7">ternational conference on machine learning </span><span class="c16 c8">(pp. 1050-1059).</span></p><p class="c0 c10"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c4">&nbsp;B. Efron. Bootstrap methods: Another look at the jackknife. Ann. Statist., 7(1):1&ndash;26, 01 1979.<br>Br. Efron. Second thoughts on the bootstrap. Statist. Sci., 18(2):135&ndash; 140, 05 2003.</span></p><p class="c0 c10"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c4">&nbsp;Blei, David M., Alp Kucukelbir, and Jon D. McAuliffe. &ldquo;Variational Inference: A Review for Statisticians.&rdquo; Journal of the American Statistical Association 112.518 (2017): 859&ndash;877. Crossref. Web.</span></p><p class="c0 c10"><span class="c4"></span></p><p class="c0 c10"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c4">&nbsp;Monte Carlo Sampling Methods using Markov Chains and their Applications. Biometrika, 57(1):97&ndash;109, April 1970.</span></p><p class="c0 c10"><span class="c4"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c8">&nbsp;</span><span class="c8 c16">Hoffman, Matthew D., and Andrew Gelman. &quot;The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.&quot; </span><span class="c7">J. Mach. Learn. Res.</span><span class="c6">&nbsp;15.1 (2014): 1593-1623.</span></p><p class="c0 c10"><span class="c6"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c8">&nbsp;</span><span class="c16 c8">Foreman-Mackey, Daniel, et al. &quot;emcee: the MCMC hammer.&quot; </span><span class="c7">Publications of the Astronomical Society of the Pacific</span><span class="c6">&nbsp;125.925 (2013): 306.</span></p><p class="c0 c10"><span class="c6"></span></p></div><div><p class="c0 c12"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c4">&nbsp;Maclaurin, Dougal, and Ryan Prescott Adams. &quot;Firefly Monte Carlo: Exact MCMC with subsets of data.&quot; Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.</span></p></div></body></html>